{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hdc\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 28*28])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义权重W和偏置bias\n",
    "W = tf.Variable(tf.zeros([28*28, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.cast将bool数组投射到float数组，用于计算平均值\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个多层卷积网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权重初始化  \n",
    "为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于使用的ReLU激活函数$f(x)=\\max(0,x)$,为了避免神经元节点输出恒为0的问题(dead neurons)。为了不在建立模型的时候反复做初始化操作，定义两个函数用于初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积和池化  \n",
    "TensorFlow在卷积和池化上有很强的灵活性。在这个实例里，我们会一直使用vanilla版本，即卷积核使用1步长，0边距，保证输出和输入是同一个大小。池化使用$2 \\times 2$大小的模板做max pooling。为了代码更灵活，我们把这部分抽象成一个函数。  \n",
    "(1)same  \n",
    "out_height=ceil(float(in_height))/float(strides[1])  \n",
    "out_width=ceil(float(in_width))/float(strides[2])  \n",
    "(2)valid  \n",
    "out_height=ceil(float(in_height-filter_height+1))/float(strides[1])  \n",
    "out_width=ceil(float(in_width-filter_width+1))/float(strides[2])  \n",
    "————————————————  \n",
    "版权声明：本文为CSDN博主「梦的漂流瓶」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。  \n",
    "原文链接：https://blog.csdn.net/qq_32466233/article/details/81075288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一层卷积  \n",
    "现在我们开始实现第一层卷积。它由一个卷积接一个max pooling完成。卷积在每个$5 \\times 5$的patch中计算32个特征。卷积的权重张量形状是[5,5,1,32]#filter_height,filter_width,input_channels,output_channels。而对于每一个输出通道都有一个对应的偏置量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了用这一层，我们把$x$变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把x_image和权重向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二层卷积  \n",
    "为了构架一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个$5 \\times 5$的patch会得到64个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 密集连接层  \n",
    "现在，图片尺寸减小到$7\\times7$，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一维向量，乘上权重矩阵，加上偏置，然后对其使用ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout  \n",
    "为了减少过拟合，我们在输出层之前加入dropout, 我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-0eb814f49c23>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输出层  \n",
    "最后，我们添加一个softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练和评估模型  \n",
    "用更加复杂的Adam优化器做梯度最速下降，在feed_dict中加入额外的keep_prob来控制dropout比例，然后每100次迭代输出一次日志."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train_accuracy: 0.120000\n",
      "step: 100, train_accuracy: 0.800000\n",
      "step: 200, train_accuracy: 0.880000\n",
      "step: 300, train_accuracy: 0.900000\n",
      "step: 400, train_accuracy: 0.940000\n",
      "step: 500, train_accuracy: 0.860000\n",
      "step: 600, train_accuracy: 0.920000\n",
      "step: 700, train_accuracy: 0.960000\n",
      "step: 800, train_accuracy: 0.820000\n",
      "step: 900, train_accuracy: 0.880000\n",
      "step: 1000, train_accuracy: 0.920000\n",
      "step: 1100, train_accuracy: 0.940000\n",
      "step: 1200, train_accuracy: 1.000000\n",
      "step: 1300, train_accuracy: 0.940000\n",
      "step: 1400, train_accuracy: 0.920000\n",
      "step: 1500, train_accuracy: 0.940000\n",
      "step: 1600, train_accuracy: 0.920000\n",
      "step: 1700, train_accuracy: 0.940000\n",
      "step: 1800, train_accuracy: 0.980000\n",
      "step: 1900, train_accuracy: 0.940000\n",
      "step: 2000, train_accuracy: 0.960000\n",
      "step: 2100, train_accuracy: 1.000000\n",
      "step: 2200, train_accuracy: 0.980000\n",
      "step: 2300, train_accuracy: 1.000000\n",
      "step: 2400, train_accuracy: 0.980000\n",
      "step: 2500, train_accuracy: 0.980000\n",
      "step: 2600, train_accuracy: 0.960000\n",
      "step: 2700, train_accuracy: 0.960000\n",
      "step: 2800, train_accuracy: 0.960000\n",
      "step: 2900, train_accuracy: 0.940000\n",
      "step: 3000, train_accuracy: 1.000000\n",
      "step: 3100, train_accuracy: 0.940000\n",
      "step: 3200, train_accuracy: 0.980000\n",
      "step: 3300, train_accuracy: 0.980000\n",
      "step: 3400, train_accuracy: 0.960000\n",
      "step: 3500, train_accuracy: 0.940000\n",
      "step: 3600, train_accuracy: 0.980000\n",
      "step: 3700, train_accuracy: 0.960000\n",
      "step: 3800, train_accuracy: 0.980000\n",
      "step: 3900, train_accuracy: 0.980000\n",
      "step: 4000, train_accuracy: 0.980000\n",
      "step: 4100, train_accuracy: 0.960000\n",
      "step: 4200, train_accuracy: 0.960000\n",
      "step: 4300, train_accuracy: 0.960000\n",
      "step: 4400, train_accuracy: 1.000000\n",
      "step: 4500, train_accuracy: 0.980000\n",
      "step: 4600, train_accuracy: 0.960000\n",
      "step: 4700, train_accuracy: 0.980000\n",
      "step: 4800, train_accuracy: 0.960000\n",
      "step: 4900, train_accuracy: 1.000000\n",
      "step: 5000, train_accuracy: 1.000000\n",
      "step: 5100, train_accuracy: 1.000000\n",
      "step: 5200, train_accuracy: 1.000000\n",
      "step: 5300, train_accuracy: 1.000000\n",
      "step: 5400, train_accuracy: 1.000000\n",
      "step: 5500, train_accuracy: 0.980000\n",
      "step: 5600, train_accuracy: 1.000000\n",
      "step: 5700, train_accuracy: 1.000000\n",
      "step: 5800, train_accuracy: 1.000000\n",
      "step: 5900, train_accuracy: 0.980000\n",
      "step: 6000, train_accuracy: 0.980000\n",
      "step: 6100, train_accuracy: 0.980000\n",
      "step: 6200, train_accuracy: 1.000000\n",
      "step: 6300, train_accuracy: 0.980000\n",
      "step: 6400, train_accuracy: 0.980000\n",
      "step: 6500, train_accuracy: 0.980000\n",
      "step: 6600, train_accuracy: 0.980000\n",
      "step: 6700, train_accuracy: 0.980000\n",
      "step: 6800, train_accuracy: 1.000000\n",
      "step: 6900, train_accuracy: 1.000000\n",
      "step: 7000, train_accuracy: 1.000000\n",
      "step: 7100, train_accuracy: 1.000000\n",
      "step: 7200, train_accuracy: 1.000000\n",
      "step: 7300, train_accuracy: 0.980000\n",
      "step: 7400, train_accuracy: 1.000000\n",
      "step: 7500, train_accuracy: 0.980000\n",
      "step: 7600, train_accuracy: 0.980000\n",
      "step: 7700, train_accuracy: 1.000000\n",
      "step: 7800, train_accuracy: 1.000000\n",
      "step: 7900, train_accuracy: 0.980000\n",
      "step: 8000, train_accuracy: 1.000000\n",
      "step: 8100, train_accuracy: 0.980000\n",
      "step: 8200, train_accuracy: 1.000000\n",
      "step: 8300, train_accuracy: 1.000000\n",
      "step: 8400, train_accuracy: 0.980000\n",
      "step: 8500, train_accuracy: 0.940000\n",
      "step: 8600, train_accuracy: 1.000000\n",
      "step: 8700, train_accuracy: 1.000000\n",
      "step: 8800, train_accuracy: 0.960000\n",
      "step: 8900, train_accuracy: 1.000000\n",
      "step: 9000, train_accuracy: 1.000000\n",
      "step: 9100, train_accuracy: 1.000000\n",
      "step: 9200, train_accuracy: 1.000000\n",
      "step: 9300, train_accuracy: 0.980000\n",
      "step: 9400, train_accuracy: 1.000000\n",
      "step: 9500, train_accuracy: 1.000000\n",
      "step: 9600, train_accuracy: 1.000000\n",
      "step: 9700, train_accuracy: 1.000000\n",
      "step: 9800, train_accuracy: 0.980000\n",
      "step: 9900, train_accuracy: 1.000000\n",
      "step: 10000, train_accuracy: 1.000000\n",
      "step: 10100, train_accuracy: 0.980000\n",
      "step: 10200, train_accuracy: 0.960000\n",
      "step: 10300, train_accuracy: 1.000000\n",
      "step: 10400, train_accuracy: 1.000000\n",
      "step: 10500, train_accuracy: 1.000000\n",
      "step: 10600, train_accuracy: 1.000000\n",
      "step: 10700, train_accuracy: 1.000000\n",
      "step: 10800, train_accuracy: 1.000000\n",
      "step: 10900, train_accuracy: 0.980000\n",
      "step: 11000, train_accuracy: 1.000000\n",
      "step: 11100, train_accuracy: 1.000000\n",
      "step: 11200, train_accuracy: 0.980000\n",
      "step: 11300, train_accuracy: 1.000000\n",
      "step: 11400, train_accuracy: 1.000000\n",
      "step: 11500, train_accuracy: 0.980000\n",
      "step: 11600, train_accuracy: 0.980000\n",
      "step: 11700, train_accuracy: 0.980000\n",
      "step: 11800, train_accuracy: 1.000000\n",
      "step: 11900, train_accuracy: 1.000000\n",
      "step: 12000, train_accuracy: 1.000000\n",
      "step: 12100, train_accuracy: 1.000000\n",
      "step: 12200, train_accuracy: 1.000000\n",
      "step: 12300, train_accuracy: 1.000000\n",
      "step: 12400, train_accuracy: 0.980000\n",
      "step: 12500, train_accuracy: 0.980000\n",
      "step: 12600, train_accuracy: 1.000000\n",
      "step: 12700, train_accuracy: 0.960000\n",
      "step: 12800, train_accuracy: 1.000000\n",
      "step: 12900, train_accuracy: 0.980000\n",
      "step: 13000, train_accuracy: 1.000000\n",
      "step: 13100, train_accuracy: 1.000000\n",
      "step: 13200, train_accuracy: 1.000000\n",
      "step: 13300, train_accuracy: 0.960000\n",
      "step: 13400, train_accuracy: 1.000000\n",
      "step: 13500, train_accuracy: 1.000000\n",
      "step: 13600, train_accuracy: 0.980000\n",
      "step: 13700, train_accuracy: 1.000000\n",
      "step: 13800, train_accuracy: 1.000000\n",
      "step: 13900, train_accuracy: 0.980000\n",
      "step: 14000, train_accuracy: 1.000000\n",
      "step: 14100, train_accuracy: 1.000000\n",
      "step: 14200, train_accuracy: 1.000000\n",
      "step: 14300, train_accuracy: 0.960000\n",
      "step: 14400, train_accuracy: 1.000000\n",
      "step: 14500, train_accuracy: 1.000000\n",
      "step: 14600, train_accuracy: 1.000000\n",
      "step: 14700, train_accuracy: 0.980000\n",
      "step: 14800, train_accuracy: 1.000000\n",
      "step: 14900, train_accuracy: 0.980000\n",
      "step: 15000, train_accuracy: 1.000000\n",
      "step: 15100, train_accuracy: 1.000000\n",
      "step: 15200, train_accuracy: 1.000000\n",
      "step: 15300, train_accuracy: 1.000000\n",
      "step: 15400, train_accuracy: 1.000000\n",
      "step: 15500, train_accuracy: 1.000000\n",
      "step: 15600, train_accuracy: 0.980000\n",
      "step: 15700, train_accuracy: 1.000000\n",
      "step: 15800, train_accuracy: 1.000000\n",
      "step: 15900, train_accuracy: 1.000000\n",
      "step: 16000, train_accuracy: 1.000000\n",
      "step: 16100, train_accuracy: 1.000000\n",
      "step: 16200, train_accuracy: 1.000000\n",
      "step: 16300, train_accuracy: 1.000000\n",
      "step: 16400, train_accuracy: 0.980000\n",
      "step: 16500, train_accuracy: 1.000000\n",
      "step: 16600, train_accuracy: 1.000000\n",
      "step: 16700, train_accuracy: 1.000000\n",
      "step: 16800, train_accuracy: 1.000000\n",
      "step: 16900, train_accuracy: 1.000000\n",
      "step: 17000, train_accuracy: 1.000000\n",
      "step: 17100, train_accuracy: 0.980000\n",
      "step: 17200, train_accuracy: 1.000000\n",
      "step: 17300, train_accuracy: 1.000000\n",
      "step: 17400, train_accuracy: 1.000000\n",
      "step: 17500, train_accuracy: 0.980000\n",
      "step: 17600, train_accuracy: 1.000000\n",
      "step: 17700, train_accuracy: 1.000000\n",
      "step: 17800, train_accuracy: 1.000000\n",
      "step: 17900, train_accuracy: 1.000000\n",
      "step: 18000, train_accuracy: 1.000000\n",
      "step: 18100, train_accuracy: 1.000000\n",
      "step: 18200, train_accuracy: 0.980000\n",
      "step: 18300, train_accuracy: 1.000000\n",
      "step: 18400, train_accuracy: 1.000000\n",
      "step: 18500, train_accuracy: 1.000000\n",
      "step: 18600, train_accuracy: 1.000000\n",
      "step: 18700, train_accuracy: 1.000000\n",
      "step: 18800, train_accuracy: 1.000000\n",
      "step: 18900, train_accuracy: 1.000000\n",
      "step: 19000, train_accuracy: 1.000000\n",
      "step: 19100, train_accuracy: 1.000000\n",
      "step: 19200, train_accuracy: 1.000000\n",
      "step: 19300, train_accuracy: 1.000000\n",
      "step: 19400, train_accuracy: 1.000000\n",
      "step: 19500, train_accuracy: 1.000000\n",
      "step: 19600, train_accuracy: 1.000000\n",
      "step: 19700, train_accuracy: 1.000000\n",
      "step: 19800, train_accuracy: 1.000000\n",
      "step: 19900, train_accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        print(\"step: %d, train_accuracy: %.6f\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy: %.6f\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
